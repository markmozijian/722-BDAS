{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Must be included at the beginning of each new notebook. Remember to change the app name.\n",
    "import findspark\n",
    "findspark.init('/home/ubuntu/spark-2.1.1-bin-hadoop2.7')\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('interation4').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Importing data which has a header. Schema is automatically configured.\n",
    "dataset = spark.read.csv('usa-stackoverflow-iteration4.csv', header=True, inferSchema=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+----------+-------+------------------+---------------+--------------+-----------+-----------+---------------+---------------+---------------+--------------+-----------+--------+-------------+------------+---------+--------+---+\n",
      "|Respondent|Hobby|OpenSource|Student|        Employment|FormalEducation|UndergradMajor|CompanySize|YearsCoding|YearsCodingProf|JobSatisfaction|ConvertedSalary|NumberMonitors|CheckInCode|WakeTime|HoursComputer|HoursOutside|SkipMeals|Exercise|Age|\n",
      "+----------+-----+----------+-------+------------------+---------------+--------------+-----------+-----------+---------------+---------------+---------------+--------------+-----------+--------+-------------+------------+---------+--------+---+\n",
      "|         1|  Yes|       Yes|     No|Employed full-time|             Nd|            Cd|          8|          4|              1|              1|         120000|             2|          5|       6|            4|           0|        1|       0|  2|\n",
      "|         2|  Yes|       Yes|     No|Employed full-time|             Nd|            Ar|          4|         11|              8|              2|         250000|             1|          5|       5|            3|           0|        1|       0|  4|\n",
      "|         3|  Yes|        No|     No|Employed full-time|             Bd|          null|          5|          9|              9|              3|           null|             3|          5|       3|            2|           2|     null|       3|  5|\n",
      "|         4|  Yes|        No|     No|Employed full-time|             Nd|          null|          2|          7|              7|             -1|          75000|             3|          5|       4|            2|           0|        3|       0|  5|\n",
      "|         5|   No|        No|     No|Employed full-time|             Bd|            Ss|          4|          2|              1|              2|         900000|             2|          3|       4|            2|           1|        1|       0|  3|\n",
      "|         6|  Yes|        No|     No|Employed full-time|             Bd|            We|          5|          3|              1|             -3|          44000|             2|          1|       2|            2|           1|        0|       3|  2|\n",
      "|         7|   No|        No|     No|Employed full-time|             Bd|            Cd|          6|          2|              2|             -2|          60000|             2|          3|       5|            3|           0|        1|       0|  3|\n",
      "|         8|  Yes|       Yes|     No|Employed full-time|             Bd|            Cd|          3|          5|              3|              2|          80000|             2|          5|       4|            2|           1|        3|       3|  3|\n",
      "|         9|  Yes|       Yes|     No|Employed full-time|             Bd|            Ae|          7|          4|              3|              3|          74000|             1|          5|       4|            3|           0|        0|       0|  3|\n",
      "|        10|  Yes|       Yes|     No|Employed full-time|             Md|            Cd|          3|          6|              4|              1|         115000|             2|          5|       3|            3|           1|        0|       2|  3|\n",
      "+----------+-----+----------+-------+------------------+---------------+--------------+-----------+-----------+---------------+---------------+---------------+--------------+-----------+--------+-------------+------------+---------+--------+---+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Let's see the data. You'll notice nulls.\n",
    "dataset.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Respondent',\n",
       " 'Hobby',\n",
       " 'OpenSource',\n",
       " 'Student',\n",
       " 'Employment',\n",
       " 'FormalEducation',\n",
       " 'UndergradMajor',\n",
       " 'CompanySize',\n",
       " 'YearsCoding',\n",
       " 'YearsCodingProf',\n",
       " 'JobSatisfaction',\n",
       " 'ConvertedSalary',\n",
       " 'NumberMonitors',\n",
       " 'CheckInCode',\n",
       " 'WakeTime',\n",
       " 'HoursComputer',\n",
       " 'HoursOutside',\n",
       " 'SkipMeals',\n",
       " 'Exercise',\n",
       " 'Age']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+-----+----------+--------------+------------------+---------------+--------------+------------------+------------------+------------------+------------------+-----------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+\n",
      "|summary|        Respondent|Hobby|OpenSource|       Student|        Employment|FormalEducation|UndergradMajor|       CompanySize|       YearsCoding|   YearsCodingProf|   JobSatisfaction|  ConvertedSalary|    NumberMonitors|       CheckInCode|          WakeTime|     HoursComputer|      HoursOutside|         SkipMeals|          Exercise|               Age|\n",
      "+-------+------------------+-----+----------+--------------+------------------+---------------+--------------+------------------+------------------+------------------+------------------+-----------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+\n",
      "|  count|             13832|13832|     13832|         13786|             13789|          13743|         12701|             12189|             13828|             13562|             13043|            12913|             13551|             13241|             13473|             13465|             13456|             13459|             13479|             13151|\n",
      "|   mean|            6916.5| null|      null|          null|              null|           null|          null| 4.792681926327016| 4.709574775817183|3.5833210440937915| 1.224641570190907|      153974.6010|2.1442697955870416| 4.198021297485084|3.3930082386996214| 2.748013367991088|1.0932669441141498|0.5849617356415782|1.2283552192299132|3.3033989810660787|\n",
      "| stddev|3993.0987966740818| null|      null|          null|              null|           null|          null|2.2056091640097915|2.7390505014517856| 2.573717328088561|1.7386740025592278|290549.2564210803| 0.764766204036218|1.1710502218969925|1.3422055872652883|0.6685584432030957|0.8392078330653067|0.8718444045370336|1.0624903702403212|0.9769759373550713|\n",
      "|    min|                 1|   No|        No|            No|Employed full-time|             Ad|            Ae|                 1|                 1|                 1|                -3|                0|                 1|                 0|                 0|                 0|                 0|                 0|                 0|                 1|\n",
      "|    max|             13832|  Yes|       Yes|Yes, part-time|           Retired|             Sd|            We|                 8|                11|                11|                 3|          2000000|                 5|                 5|                10|                 4|                 4|                 3|                 3|                 7|\n",
      "+-------+------------------+-----+----------+--------------+------------------+---------------+--------------+------------------+------------------+------------------+------------------+-----------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataset.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Respondent: integer (nullable = true)\n",
      " |-- Hobby: string (nullable = true)\n",
      " |-- OpenSource: string (nullable = true)\n",
      " |-- Student: string (nullable = true)\n",
      " |-- Employment: string (nullable = true)\n",
      " |-- FormalEducation: string (nullable = true)\n",
      " |-- UndergradMajor: string (nullable = true)\n",
      " |-- CompanySize: integer (nullable = true)\n",
      " |-- YearsCoding: integer (nullable = true)\n",
      " |-- YearsCodingProf: integer (nullable = true)\n",
      " |-- JobSatisfaction: integer (nullable = true)\n",
      " |-- ConvertedSalary: decimal(7,0) (nullable = true)\n",
      " |-- NumberMonitors: integer (nullable = true)\n",
      " |-- CheckInCode: integer (nullable = true)\n",
      " |-- WakeTime: integer (nullable = true)\n",
      " |-- HoursComputer: integer (nullable = true)\n",
      " |-- HoursOutside: integer (nullable = true)\n",
      " |-- SkipMeals: integer (nullable = true)\n",
      " |-- Exercise: integer (nullable = true)\n",
      " |-- Age: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataset.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13832"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset[dataset[\"Employment\"]==\"Employed full-time\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11995"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset[dataset[\"ConvertedSalary\"].isNotNull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11482"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "919"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import isnan\n",
    "dataset.filter((dataset[\"ConvertedSalary\"] == \"\") | dataset[\"ConvertedSalary\"].isNull() | isnan(dataset[\"ConvertedSalary\"])).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset=dataset.na.drop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9959"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "bounds = dataset.approxQuantile(\"ConvertedSalary\", [0.25, 0.75], 0)\n",
    "IQR = bounds[1]-bounds[0]\n",
    "lowerRange = bounds[0] - 1.5*IQR\n",
    "upperRange = bounds[1]+ 1.5*IQR\n",
    "dataset = dataset.filter((dataset['ConvertedSalary'] >= lowerRange) & (dataset['ConvertedSalary'] <= upperRange))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9391"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[100000.0]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.approxQuantile(\"ConvertedSalary\", [0.5], 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as f\n",
    "dataset = dataset.withColumn('NewSalaryLevel', f.when(f.col('ConvertedSalary') < 100000, 0).otherwise(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_drop = ['Student', 'Employment','ConvertedSalary','Respondent','Hobby','OpenSource']\n",
    "dataset = dataset.drop(*columns_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+--------------+-----------+-----------+---------------+---------------+--------------+-----------+--------+-------------+------------+---------+--------+---+--------------+\n",
      "|FormalEducation|UndergradMajor|CompanySize|YearsCoding|YearsCodingProf|JobSatisfaction|NumberMonitors|CheckInCode|WakeTime|HoursComputer|HoursOutside|SkipMeals|Exercise|Age|NewSalaryLevel|\n",
      "+---------------+--------------+-----------+-----------+---------------+---------------+--------------+-----------+--------+-------------+------------+---------+--------+---+--------------+\n",
      "|             Nd|            Cd|          8|          4|              1|              1|             2|          5|       6|            4|           0|        1|       0|  2|             1|\n",
      "|             Bd|            We|          5|          3|              1|             -3|             2|          1|       2|            2|           1|        0|       3|  2|             0|\n",
      "|             Bd|            Cd|          6|          2|              2|             -2|             2|          3|       5|            3|           0|        1|       0|  3|             0|\n",
      "|             Bd|            Cd|          3|          5|              3|              2|             2|          5|       4|            2|           1|        3|       3|  3|             0|\n",
      "|             Bd|            Ae|          7|          4|              3|              3|             1|          5|       4|            3|           0|        0|       0|  3|             0|\n",
      "|             Md|            Cd|          3|          6|              4|              1|             2|          5|       3|            3|           1|        0|       2|  3|             1|\n",
      "|             Ad|            Cd|          4|          6|              2|              2|             2|          3|       0|            3|           0|        0|       3|  3|             0|\n",
      "|             Bd|            Cd|          3|          7|              7|              2|             3|          5|       3|            3|           0|        0|       1|  4|             1|\n",
      "|             Md|            Cd|          8|          5|              3|              2|             2|          5|       2|            3|           3|        2|       3|  3|             1|\n",
      "|             Bd|            Hu|          1|          1|              1|              2|             2|          5|       4|            3|           1|        0|       1|  3|             0|\n",
      "+---------------+--------------+-----------+-----------+---------------+---------------+--------------+-----------+--------+-------------+------------+---------+--------+---+--------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataset.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import (VectorAssembler,VectorIndexer,\n",
    "                                OneHotEncoder,StringIndexer)\n",
    "# First create a string indexer (convert every string into a number, such as male = 0 and female = 1).\n",
    "# A number will be assigned to every category in the column.\n",
    "FormalEducation_indexer = StringIndexer(inputCol='FormalEducation',outputCol='FormalEducationIndex')\n",
    "\n",
    "# Now we can one hot encode these numbers. This converts the various outputs into a single vector.\n",
    "# This makes it easier to process when you have multiple classes.\n",
    "FormalEducation_encoder = OneHotEncoder(inputCol='FormalEducationIndex',outputCol='FormalEducationVec')\n",
    "\n",
    "UndergradMajor_indexer = StringIndexer(inputCol='UndergradMajor',outputCol='UndergradMajorIndex')\n",
    "UndergradMajor__encoder = OneHotEncoder(inputCol='UndergradMajorIndex',outputCol='UndergradMajorVec')\n",
    "\n",
    "# assemble all of this as one vector in the features column. \n",
    "assembler = VectorAssembler(inputCols=[\n",
    " 'FormalEducationVec',\n",
    " 'UndergradMajorVec',\n",
    " 'CompanySize',\n",
    " 'YearsCoding',\n",
    " 'YearsCodingProf',\n",
    " 'JobSatisfaction',\n",
    " 'NumberMonitors',\n",
    " 'CheckInCode',\n",
    " 'WakeTime',\n",
    " 'HoursComputer',\n",
    " 'HoursOutside',\n",
    " 'SkipMeals',\n",
    " 'Exercise',\n",
    " 'Age'],outputCol='features')\n",
    "\n",
    "\n",
    "# Train/test split. \n",
    "trainset, testset= dataset.randomSplit([0.8,0.2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+----------+\n",
      "|NewSalaryLevel|prediction|\n",
      "+--------------+----------+\n",
      "|             0|       0.0|\n",
      "|             0|       1.0|\n",
      "|             1|       1.0|\n",
      "|             1|       0.0|\n",
      "|             0|       0.0|\n",
      "|             1|       1.0|\n",
      "|             1|       1.0|\n",
      "|             0|       0.0|\n",
      "|             1|       0.0|\n",
      "|             0|       0.0|\n",
      "|             0|       0.0|\n",
      "|             1|       0.0|\n",
      "|             0|       0.0|\n",
      "|             1|       1.0|\n",
      "|             0|       0.0|\n",
      "|             0|       0.0|\n",
      "|             0|       0.0|\n",
      "|             0|       0.0|\n",
      "|             1|       1.0|\n",
      "|             1|       1.0|\n",
      "+--------------+----------+\n",
      "only showing top 20 rows\n",
      "\n",
      "Test Area Under ROC 0.7112227680671661\n"
     ]
    }
   ],
   "source": [
    "# Logistic Regression Model\n",
    "# Note that survived is a categorial variable but didn't require any transformation.\n",
    "# That's because it's already in the format of 1's and 0's. \n",
    "log_reg = LogisticRegression(featuresCol='features',labelCol='NewSalaryLevel')\n",
    "\n",
    "\n",
    "# Lists everything we want to do. Index data, encode data, assemble data and then pass in the actual model.\n",
    "pipeline_log = Pipeline(stages=[FormalEducation_indexer,UndergradMajor_indexer,\n",
    "                           FormalEducation_encoder,UndergradMajor__encoder,\n",
    "                           assembler,log_reg])\n",
    "\n",
    "# Note pipeline. Call it as you would call a machine learning object.\n",
    "fit_model = pipeline_log.fit(trainset)\n",
    "\n",
    "# Transform test data. \n",
    "predictions_log_reg = fit_model.transform(testset)\n",
    "\n",
    "# Evaluate the model using the binary classifer.\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "my_eval= BinaryClassificationEvaluator(rawPredictionCol='prediction',\n",
    "                                       labelCol='NewSalaryLevel')\n",
    "\n",
    "\n",
    "# If we select the actual and predicted results, we can see that some predictions were correct while others were wrong.\n",
    "predictions_log_reg.select('NewSalaryLevel','prediction').show()\n",
    "\n",
    "\n",
    "# We can then evaluate using AUC (area under the curve). AUC is linked to ROC.\n",
    "print('Test Area Under ROC', my_eval.evaluate(predictions_log_reg ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+----------+\n",
      "|NewSalaryLevel|prediction|\n",
      "+--------------+----------+\n",
      "|             0|       0.0|\n",
      "|             0|       1.0|\n",
      "|             1|       0.0|\n",
      "|             1|       0.0|\n",
      "|             0|       0.0|\n",
      "|             1|       0.0|\n",
      "|             1|       1.0|\n",
      "|             0|       0.0|\n",
      "|             1|       0.0|\n",
      "|             0|       1.0|\n",
      "|             0|       0.0|\n",
      "|             1|       1.0|\n",
      "|             0|       1.0|\n",
      "|             1|       0.0|\n",
      "|             0|       0.0|\n",
      "|             0|       0.0|\n",
      "|             0|       0.0|\n",
      "|             0|       0.0|\n",
      "|             1|       0.0|\n",
      "|             1|       0.0|\n",
      "+--------------+----------+\n",
      "only showing top 20 rows\n",
      "\n",
      "Test Area Under ROC 0.6910826728911085\n"
     ]
    }
   ],
   "source": [
    "# Decision Tree Model\n",
    "\n",
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "decision_tree = DecisionTreeClassifier(featuresCol='features',labelCol='NewSalaryLevel')\n",
    "\n",
    "\n",
    "# Lists everything we want to do. Index data, encode data, assemble data and then pass in the actual model.\n",
    "pipeline_dec = Pipeline(stages=[FormalEducation_indexer,UndergradMajor_indexer,\n",
    "                           FormalEducation_encoder,UndergradMajor__encoder,\n",
    "                           assembler,decision_tree])\n",
    "\n",
    "# Note pipeline. Call it as you would call a machine learning object.\n",
    "fit_model_dec = pipeline_dec.fit(trainset)\n",
    "\n",
    "# Transform test data. \n",
    "predictions_decision_tree = fit_model_dec.transform(testset)\n",
    "\n",
    "\n",
    "# If we select the actual and predicted results, we can see that some predictions were correct while others were wrong.\n",
    "predictions_decision_tree.select('NewSalaryLevel','prediction').show()\n",
    "\n",
    "\n",
    "# We can then evaluate using AUC (area under the curve). AUC is linked to ROC.\n",
    "print('Test Area Under ROC', my_eval.evaluate(predictions_decision_tree ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+----------+\n",
      "|NewSalaryLevel|prediction|\n",
      "+--------------+----------+\n",
      "|             0|       0.0|\n",
      "|             0|       1.0|\n",
      "|             1|       1.0|\n",
      "|             1|       0.0|\n",
      "|             0|       0.0|\n",
      "|             1|       1.0|\n",
      "|             1|       1.0|\n",
      "|             0|       0.0|\n",
      "|             1|       0.0|\n",
      "|             0|       1.0|\n",
      "|             0|       0.0|\n",
      "|             1|       1.0|\n",
      "|             0|       1.0|\n",
      "|             1|       1.0|\n",
      "|             0|       1.0|\n",
      "|             0|       0.0|\n",
      "|             0|       0.0|\n",
      "|             0|       1.0|\n",
      "|             1|       1.0|\n",
      "|             1|       1.0|\n",
      "+--------------+----------+\n",
      "only showing top 20 rows\n",
      "\n",
      "Test Area Under ROC 0.7119635037817102\n"
     ]
    }
   ],
   "source": [
    "# Random Forest Model\n",
    " \n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "rf = RandomForestClassifier(featuresCol='features',labelCol='NewSalaryLevel')\n",
    "\n",
    "# Lists everything we want to do. Index data, encode data, assemble data and then pass in the actual model.\n",
    "pipeline_rf = Pipeline(stages=[FormalEducation_indexer,UndergradMajor_indexer,\n",
    "                           FormalEducation_encoder,UndergradMajor__encoder,\n",
    "                           assembler,rf])\n",
    "\n",
    "# Note pipeline. Call it as you would call a machine learning object.\n",
    "fit_model_rf = pipeline_rf.fit(trainset)\n",
    "\n",
    "# Transform test data. \n",
    "predictions_rf = fit_model_rf.transform(testset)\n",
    "\n",
    "\n",
    "# If we select the actual and predicted results, we can see that some predictions were correct while others were wrong.\n",
    "predictions_rf.select('NewSalaryLevel','prediction').show()\n",
    "\n",
    "\n",
    "# We can then evaluate using AUC (area under the curve). AUC is linked to ROC.\n",
    "print('Test Area Under ROC', my_eval.evaluate(predictions_rf ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+----------+\n",
      "|NewSalaryLevel|prediction|\n",
      "+--------------+----------+\n",
      "|             0|       0.0|\n",
      "|             0|       1.0|\n",
      "|             1|       0.0|\n",
      "|             1|       1.0|\n",
      "|             0|       0.0|\n",
      "|             1|       0.0|\n",
      "|             1|       1.0|\n",
      "|             0|       0.0|\n",
      "|             1|       0.0|\n",
      "|             0|       1.0|\n",
      "|             0|       0.0|\n",
      "|             1|       0.0|\n",
      "|             0|       1.0|\n",
      "|             1|       0.0|\n",
      "|             0|       0.0|\n",
      "|             0|       0.0|\n",
      "|             0|       0.0|\n",
      "|             0|       0.0|\n",
      "|             1|       0.0|\n",
      "|             1|       0.0|\n",
      "+--------------+----------+\n",
      "only showing top 20 rows\n",
      "\n",
      "Test Area Under ROC 0.7117282777838093\n"
     ]
    }
   ],
   "source": [
    "# Gradient-boosted tree model\n",
    "from pyspark.ml.classification import GBTClassifier\n",
    "\n",
    "gbt = GBTClassifier(featuresCol='features',labelCol='NewSalaryLevel')\n",
    "\n",
    "# Lists everything we want to do. Index data, encode data, assemble data and then pass in the actual model.\n",
    "pipeline_gbt = Pipeline(stages=[FormalEducation_indexer,UndergradMajor_indexer,\n",
    "                           FormalEducation_encoder,UndergradMajor__encoder,\n",
    "                           assembler,gbt])\n",
    "\n",
    "# Note pipeline. Call it as you would call a machine learning object.\n",
    "fit_model_gbt = pipeline_gbt.fit(trainset)\n",
    "\n",
    "# Transform test data. \n",
    "predictions_gbt = fit_model_gbt.transform(testset)\n",
    "\n",
    "\n",
    "# If we select the actual and predicted results, we can see that some predictions were correct while others were wrong.\n",
    "predictions_gbt.select('NewSalaryLevel','prediction').show()\n",
    "\n",
    "\n",
    "# We can then evaluate using AUC (area under the curve). AUC is linked to ROC.\n",
    "print('Test Area Under ROC', my_eval.evaluate(predictions_gbt ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
